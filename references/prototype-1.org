#+SETUPFILE: ../reports/exports/theme-readtheorg.setup
* Prototype 1
For this first version of the program that should be ready for use in a portfolio, objectives are to select jobs depending on word apparitions, browse offers, and have some basic stats of word usage.
** [#A] Tool to look for jobs
*** [#A] static backend
**** scrapping
***** DONE check result of today 2018-09-14 ven. 20:20
CLOSED: [2018-09-15 sam. 16:22]
***** TODO launch everyday from query list
****** DONE make query list out of already made queries
CLOSED: [2018-10-05 ven. 15:31]
:LOGBOOK:
- State "DONE"       from              [2018-10-05 ven. 15:31] \\
  See Queries in notebook
:END:
****** cron
***** add source query to features
***** spped up
****** remove self set limits ?
****** server bans ?
****** vpn ?
****** tor ?
***** test : make scrap of the whole website
**** data selection
***** org : form interface 
***** DONE add tags to actual dataset / save working dataset
CLOSED: [2018-10-05 ven. 15:33]
:LOGBOOK:
- State "DONE"       from              [2018-10-05 ven. 15:33] \\
  Save datasets at different stages of processing by attributing a new variable name to it.
:END:
**** filtering
:LOGBOOK:
- State "TODO"       from "WAITING"    [2018-09-15 sam. 16:25]
- State "DONE"       from              [2018-09-13 jeu. 16:18]
:END:
***** DONE resolve ipython output
CLOSED: [2018-10-05 ven. 15:34]
:LOGBOOK:
- State "DONE"       from              [2018-10-05 ven. 15:34]
:END:
**** internal correlations
***** nlp
****** nltk
****** similarity / clustering
****** speed !
******* numpy
******* sql
******* cython
***** historic
****** redondancy / new 
****** database sorted per date
**** external correlations
***** identify big firms / small ventures
***** geolocalize headquarters on a map
**** querying system
*** [#B] front end
**** TODO queries management
***** DONE query bag
CLOSED: [2018-10-05 ven. 15:35]
:LOGBOOK:
- State "DONE"       from              [2018-10-05 ven. 15:35] \\
  Queries can be listed in files created from all made queries.
  They can then be modified and processed to filter or launch new scraper crawl.
:END:
****** simple plain text file
****** prompt : "add to query bag ?" when running a crawl
****** autofill : get all previously used queries
**** fiche job
**** analysis viz
** [#B] potfolio
*** blog : Gandi / git
**** article pr√©sentation
**** extensions futur
*** Portability
**** docker / makefile package
**** Test with a friend
