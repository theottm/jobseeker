* Design
:PROPERTIES:
:CREATED:  <2019-05-21 mar. 00:24>
:END:
** File Structure
:PROPERTIES:
:CREATED:  <2019-05-07 mar. 20:35>
:END:
*** scraper
:PROPERTIES:
:CREATED:  <2019-05-07 mar. 20:36>
:END:
*** data
:PROPERTIES:
:CREATED:  <2019-05-07 mar. 20:36>
:END:
**** a database
:PROPERTIES:
:CREATED:  <2019-05-07 mar. 20:36>
:END:
**** a collector
:PROPERTIES:
:CREATED:  <2019-05-07 mar. 20:36>
:END:
*** notebooks
:PROPERTIES:
:CREATED:  <2019-05-07 mar. 20:36>
:END:
**** experiments for publications
:PROPERTIES:
:CREATED:  <2019-05-07 mar. 20:37>
:END:
**** journaling
:PROPERTIES:
:CREATED:  <2019-05-07 mar. 20:37>
:END:
*** programms
:PROPERTIES:
:CREATED:  <2019-05-07 mar. 20:37>
:END:
**** standalone programms
:PROPERTIES:
:CREATED:  <2019-05-07 mar. 20:37>
:END:
** Data Pipeline 
:PROPERTIES:
:CREATED:  <2019-05-07 mar. 20:43>
:END:
*** Data aquisition
:PROPERTIES:
:CREATED:  <2019-05-07 mar. 20:43>
:END:
**** Scrap the whole ?
:PROPERTIES:
:CREATED:  <2019-05-07 mar. 20:43>
:END:
***** 600,000 in Germany
:PROPERTIES:
:CREATED:  <2019-05-07 mar. 20:53>
:END:
***** 7 days of scraping
:PROPERTIES:
:CREATED:  <2019-05-07 mar. 20:53>
:END:
***** scrap only recents ? 
:PROPERTIES:
:CREATED:  <2019-05-07 mar. 20:54>
:END:
****** save last date
:PROPERTIES:
:CREATED:  <2019-05-07 mar. 20:55>
:END:
****** scrap everything since then
:PROPERTIES:
:CREATED:  <2019-05-07 mar. 20:55>
:END:
****** max:  10 days
:PROPERTIES:
:CREATED:  <2019-05-07 mar. 20:55>
:END:
**** focus on one place and field
:PROPERTIES:
:CREATED:  <2019-05-07 mar. 21:01>
:END:
*** Storage
:PROPERTIES:
:CREATED:  <2019-05-07 mar. 20:57>
:END:
**** temp csv file for collector programm
:PROPERTIES:
:CREATED:  <2019-05-07 mar. 20:59>
:END:
**** database
:PROPERTIES:
:CREATED:  <2019-05-07 mar. 20:57>
:END:
* Journal
:PROPERTIES:
:CREATED:  <2019-05-21 mar. 00:24>
:END:
** 190520
:PROPERTIES:
:CREATED:  <2019-05-21 mar. 00:24>
:END:
The scraper can run a chosen amount of parallel crawls thanks to ansyncio.
A list of keywords is used to scrape job offers in Paris.
The scraper ran on keywords lists form cybordgab.

We still would like to:
1. Have a navigable listing of sorted job offers (partly done with the crawler list)
2. Explore different scoring rules
3. Translate keywords
4. Be able to chose the city / region in a country
5. Have a general overview of the words used in a given pool of jobs.
6. Be able to chose the country / or at least the spider
7. Run it on a cloud / server
** 190605 
:PROPERTIES:
:CREATED:  <2019-06-05 mer. 21:30>
:END:
Add negative keywords
New concept : [[https://en.wikipedia.org/wiki/Taxonomy][Taxonomy]]  
** 190616
:PROPERTIES:
:CREATED:  <2019-06-16 dim. 15:21>
:END:
* State:
:PROPERTIES:
:CREATED:  <2019-06-16 dim. 15:25>
:END:
#+BEGIN_SRC shell :results output
tree .. -L 3
#+END_SRC

#+RESULTS:
#+begin_example
..
├── config.ini
├── config.ini~
├── data
│   ├── cyborgab
│   │   └── 190520
│   ├── scraper_keywords.txt
│   ├── teddd
│   │   └── keywords.txt
│   ├── test
│   │   ├── 190520-2345
│   │   └── 190520-2346
│   └── usr
│       ├── 190520-2338
│       ├── 190520-2339
│       ├── 190520-2343
│       └── 190520-2344
├── jobseeker_launcher.py
├── jobseeker.py~
├── notebooks
│   ├── #analysis.org#
│   ├── analysis.org
│   ├── analysis.py
│   ├── analysis.py~
│   ├── jobseeker_archive.org
│   ├── #journal.org#
│   ├── journal.org
│   └── journal.org~
├── products
│   └── keywords_ranking
│       ├── 190521-2204-rank.csv
│       ├── 190524-0116-rank.csv
│       ├── 190524-0117-rank.csv
│       ├── 190524-0119-rank.csv
│       └── html
└── programms
    ├── __init__.py
    ├── project_build
    ├── __pycache__
    │   ├── __init__.cpython-37.pyc
    │   └── utils.cpython-37.pyc
    ├── scraper
    │   └── indeed_scraper
    ├── utils
    │   ├── __init__.py
    │   ├── __pycache__
    │   ├── scraper_wrapper.sh
    │   ├── scraper_wrapper.sh~
    │   ├── utils.py
    │   └── utils.py~
    └── viewer
        ├── classement-offres-190522.zip
        ├── html_generator.py
        ├── html_generator.py~
        ├── index_template.html
        ├── index_template.html~
        ├── index_template.html.bak
        ├── job-offers-ranking-cyborgab-190522.zip
        ├── job_template.html
        └── job_template.html~

24 directories, 35 files
#+end_example
