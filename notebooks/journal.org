* Design
:PROPERTIES:
:CREATED:  <2019-05-21 mar. 00:24>
:END:
** File Structure
:PROPERTIES:
:CREATED:  <2019-05-07 mar. 20:35>
:END:
*** scraper
:PROPERTIES:
:CREATED:  <2019-05-07 mar. 20:36>
:END:
*** data
:PROPERTIES:
:CREATED:  <2019-05-07 mar. 20:36>
:END:
**** a database
:PROPERTIES:
:CREATED:  <2019-05-07 mar. 20:36>
:END:
**** a collector
:PROPERTIES:
:CREATED:  <2019-05-07 mar. 20:36>
:END:
*** notebooks
:PROPERTIES:
:CREATED:  <2019-05-07 mar. 20:36>
:END:
**** experiments for publications
:PROPERTIES:
:CREATED:  <2019-05-07 mar. 20:37>
:END:
**** journaling
:PROPERTIES:
:CREATED:  <2019-05-07 mar. 20:37>
:END:
*** programms
:PROPERTIES:
:CREATED:  <2019-05-07 mar. 20:37>
:END:
**** standalone programms
:PROPERTIES:
:CREATED:  <2019-05-07 mar. 20:37>
:END:
** Data Pipeline 
:PROPERTIES:
:CREATED:  <2019-05-07 mar. 20:43>
:END:
*** Data aquisition
:PROPERTIES:
:CREATED:  <2019-05-07 mar. 20:43>
:END:
**** Scrap the whole ?
:PROPERTIES:
:CREATED:  <2019-05-07 mar. 20:43>
:END:
***** 600,000 in Germany
:PROPERTIES:
:CREATED:  <2019-05-07 mar. 20:53>
:END:
***** 7 days of scraping
:PROPERTIES:
:CREATED:  <2019-05-07 mar. 20:53>
:END:
***** scrap only recents ? 
:PROPERTIES:
:CREATED:  <2019-05-07 mar. 20:54>
:END:
****** save last date
:PROPERTIES:
:CREATED:  <2019-05-07 mar. 20:55>
:END:
****** scrap everything since then
:PROPERTIES:
:CREATED:  <2019-05-07 mar. 20:55>
:END:
****** max:  10 days
:PROPERTIES:
:CREATED:  <2019-05-07 mar. 20:55>
:END:
**** focus on one place and field
:PROPERTIES:
:CREATED:  <2019-05-07 mar. 21:01>
:END:
*** Storage
:PROPERTIES:
:CREATED:  <2019-05-07 mar. 20:57>
:END:
**** temp csv file for collector programm
:PROPERTIES:
:CREATED:  <2019-05-07 mar. 20:59>
:END:
**** database
:PROPERTIES:
:CREATED:  <2019-05-07 mar. 20:57>
:END:
* Journal
:PROPERTIES:
:CREATED:  <2019-05-21 mar. 00:24>
:END:
** 190520
:PROPERTIES:
:CREATED:  <2019-05-21 mar. 00:24>
:END:
The scraper can run a chosen amount of parallel crawls thanks to ansyncio.
A list of keywords is used to scrape job offers in Paris.
The scraper ran on keywords lists form cybordgab.

We still would like to:
1. Have a navigable listing of sorted job offers (partly done with the crawler list)
2. Explore different scoring rules
3. Translate keywords
4. Be able to chose the city / region in a country
5. Have a general overview of the words used in a given pool of jobs.
6. Be able to chose the country / or at least the spider
7. Run it on a cloud / server
** 190605 
:PROPERTIES:
:CREATED:  <2019-06-05 mer. 21:30>
:END:
Add negative keywords
New concept : [[https://en.wikipedia.org/wiki/Taxonomy][Taxonomy]]  
** 190617
:PROPERTIES:
:CREATED:  <2019-06-16 dim. 15:21>
:END:
updated tasks:
- run from a windows machine
- parallel run for ranking programm
- use groups of keywords in the ranking list for modular ranking
- negative keywords
- find a translation service
- try the "whole country crawl" (https://de.indeed.com/Jobs-in-Deutschland; https://www.indeed.ch/Stellen-in-Switzerland; https://www.indeed.fr/Emplois-job; https://se.indeed.com/jobb-i-Sverige)
- cluster analysis
- run the next version on the cloud
* State:
:PROPERTIES:
:CREATED:  <2019-06-16 dim. 15:25>
:END:
#+BEGIN_SRC shell :results output
tree .. -L 3
#+END_SRC

#+RESULTS:
#+begin_example
..
├── config.ini
├── config.ini~
├── data
│   ├── cyborgab
│   │   ├── output
│   │   └── scraper_keywords.txt
│   └── teddd
│       ├── output
│       ├── scraper_keywords.bak
│       ├── scraper_keywords.txt
│       └── scraper_keywords.txt~
├── jobseeker_launcher.py
├── jobseeker_launcher.py~
├── jobseeker.py~
├── notebooks
│   ├── analysis.org
│   ├── jobseeker_archive.org
│   ├── journal.org
│   └── journal.org~
├── products
│   └── keywords_ranking
│       ├── 190521-2204-rank.csv
│       ├── 190524-0116-rank.csv
│       ├── 190524-0117-rank.csv
│       ├── 190524-0119-rank.csv
│       ├── html
│       └── ranking.csv
└── programms
    ├── __init__.py
    ├── __pycache__
    │   ├── __init__.cpython-37.pyc
    │   └── utils.cpython-37.pyc
    ├── ranking.py
    ├── scraper
    │   └── indeed_scraper
    ├── utils
    │   ├── __init__.py
    │   ├── __pycache__
    │   ├── scraper_wrapper.sh
    │   ├── scraper_wrapper.sh~
    │   ├── utils.py
    │   └── utils.py~
    └── viewer
        ├── html_generator.py
        ├── html_generator.py~
        ├── index_template.html
        ├── index_template.html~
        ├── index_template.html.bak
        ├── job-offers-ranking-cyborgab-190522.zip
        ├── job_template.html
        └── job_template.html~

16 directories, 35 files
#+end_example
