#+SETUPFILE: ../reports/exports/theme-readtheorg.setup
* Explorer
  :PROPERTIES:
  :header-args: :session explorer :results raw drawer
  :END:
Proper program.
** Imports
*** ipython
#+BEGIN_SRC ipython
  %matplotlib inline
  import matplotlib.pyplot as plt
  import numpy as np
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[1]:
:END:
*** pandas
#+BEGIN_SRC ipython
import pandas as pd    
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[2]:
:END:

** Data load
*** load everything
**** file list with path
#+BEGIN_SRC ipython
import os
csv_files = []
date = "2018-09-30"
for dirpath, dirs, files in os.walk("../data/raw/" + date): 
  for filename in files:
    fname = os.path.join(dirpath,filename)
    if fname.endswith('.csv'):
      csv_files.append(fname)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[49]:
:END:
**** dataframe creation
#+BEGIN_SRC ipython
    jobs = pd.DataFrame()

    for fl in csv_files:
        print(fl+(30-len(fl)//2)*" *")
        try:
            jobs_set = pd.read_csv(fl)
            jobs_set.dropna(axis=0, how='any', subset=["desc"], inplace=True)
            jobs_set.drop_duplicates(subset="desc", inplace=True)            
            try:                                                             
                jobs.iloc[0,0]                                               
                jobs = jobs.append(jobs_set)                                 
            except IndexError:                                               
                jobs = jobs_set                                              
        except pd.errors.EmptyDataError:
            pass
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[50]:
:END:

**** TODO time range selection
*** rename
use to quickly reset original df
#+BEGIN_SRC ipython
df = jobs
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[51]:
:END:

*** python example                                                  :test:
#+NAME: firstblock
#+BEGIN_SRC python
    x = 12
    return x
#+END_SRC

#+BEGIN_SRC python :var x=firstblock
return int(x)+1
#+END_SRC

*** org doc elisp example                                           :test:
#+NAME: example-table
| 1 |
| 2 |
| 3 |
| 4 |

*** python                                                            :python:
#+NAME: data-path
#+BEGIN_SRC python :results value file
"~/data/projects/jobseeker/data/raw/18-09-07/dsp.csv"
#+END_SRC

#+RESULTS[d5047aa3d26b44e4cd843798c5ad30431cd8fc49]: data-path
[[file:None]]

#+NAME: data-dsp
#+BEGIN_SRC python :results value file
"~/data/projects/jobseeker/data/raw/18-09-07/dsp.csv"
#+END_SRC

#+RESULTS[d5047aa3d26b44e4cd843798c5ad30431cd8fc49]: data-dsp
[[file:None]]

#+NAME: data-python
#+BEGIN_SRC python :results value file
"~/data/projects/jobseeker/data/raw/18-09-07/python.csv"
#+END_SRC

#+RESULTS[f247fbba660ab3bb4061ef0d92294fd713d146b4]: data-python
[[file:None]]

#+NAME: data-ds
#+BEGIN_SRC python :results value file
"~/data/projects/jobseeker/data/raw/18-09-07/data scientist.csv"
#+END_SRC

#+RESULTS[24df27fa52b775d0702292eb7c6a390d2bcd9717]: data-ds
[[file:None]]

#+NAME: data-se
#+BEGIN_SRC python :results value file
"~/data/projects/jobseeker/data/raw/18-09-07/software engineer.csv"
#+END_SRC

#+RESULTS[bd39a59c82b8b878b05fac0296a88e2e3182efd4]: data-se
[[file:None]]
** Cleansing / Formating                                                :clean:
*** duplicates
**** drop_duplicates
#+BEGIN_SRC ipython
    df.drop_duplicates(subset="desc", inplace=True)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[52]:
:END:

**** count
#+BEGIN_SRC ipython
df.title.count()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[53]:
: 1837
:END:

*** olders
**** map lambda                                                        :test:
#+BEGIN_SRC ipython
df = df[df.days_ago.str.contains("30+").map(lambda x: not x)]
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[8]:
:END:

**** ==False
#+BEGIN_SRC ipython
df = df[df.days_ago.str.contains("30+")==False]
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[54]:
:END:

**** count
#+BEGIN_SRC ipython
len(df)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[28]:
: 934
:END:

*** string numbers to integers
#+BEGIN_SRC ipython
    df["days_ago"] = df.days_ago.apply(lambda x: int(x))
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[39]:
:END:
*** drop erratic values
**** run 
#+BEGIN_SRC ipython
    df = df[df.days_ago.lt(30)]
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[81]:
:END:
**** tests
#+BEGIN_SRC ipython
    df.days_ago.lt(30)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[80]:
#+BEGIN_EXAMPLE
  3      True
  12     True
  14     True
  15     True
  19     True
  23     True
  27     True
  28     True
  35     True
  38     True
  45     True
  48     True
  55     True
  57     True
  59     True
  62     True
  63     True
  64     True
  65     True
  66     True
  75     True
  79     True
  82     True
  87     True
  91     True
  92     True
  93     True
  94     True
  96     True
  100    True
  ...
  44     True
  46     True
  49     True
  54     True
  55     True
  65     True
  68     True
  69     True
  70     True
  74     True
  77     True
  82     True
  84     True
  87     True
  89     True
  90     True
  93     True
  95     True
  96     True
  97     True
  102    True
  105    True
  109    True
  115    True
  116    True
  119    True
  121    True
  124    True
  126    True
  2      True
  Name: days_ago, Length: 1625, dtype: bool
#+END_EXAMPLE
:END:
*** rename
#+BEGIN_SRC ipython
df_clean = df
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[55]:
:END:
** Queries
*** get queries metadata
**** dataframe using os results
#+BEGIN_SRC ipython
import os
queries_name = []
queries_size = []
queries_path = []
queries_time = []
for dirpath, dirs, files in os.walk("../data/raw"): 
  for filename in files:
    if filename.endswith('.csv'):
      
      path = os.path.join(dirpath, filename)
      queries_path.append(path)
      
      size = os.path.getsize(path)
      queries_size.append(size)
      
      fname = filename.replace(".csv", "")
      queries_name.append(fname)
      
      time = os.path.getmtime(path)
      queries_time.append(time)

queries = pd.DataFrame({"name" : queries_name, "path" : queries_path, "size" : queries_size, "time" : queries_time})      
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[33]:
:END:
**** remove oldests results 
***** datetime time format
#+BEGIN_SRC ipython
from datetime import datetime
queries["time"] = queries.time.apply(datetime.fromtimestamp)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[34]:
:END:

***** y-m-d format time
#+BEGIN_SRC ipython
def format_time(x):
    y = x.strftime("%Y-%m-%d")
    return y

queries["time_formated"] = queries.time.apply(format_time)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[35]:
:END:
**** remove null size results 
#+BEGIN_SRC ipython
queries_null = queries[queries["size"] < 1]
queries = queries[queries["size"] > 1]
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[36]:
:END:
**** number of entries in csv file
***** read as pandas dataframe
#+BEGIN_SRC ipython
def entries_count(csv):
    return len(pd.read_csv(csv))

queries["entries"] = queries.path.apply(entries_count)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[37]:
:END:

**** inspection
#+BEGIN_SRC ipython
import humanize
queries["size_for_humans"] = queries["size"].apply(humanize.naturalsize)
queries.sort_values("size", ascending=False)[["name", "size_for_humans", "entries"]].reset_index()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[38]:
#+BEGIN_EXAMPLE
  index                         name size_for_humans  entries
  0      134                         data          3.5 MB     1363
  1      133                       python          3.4 MB     1291
  2      121                 intelligence          3.2 MB     1108
  3      126                       pyjobs          3.2 MB     1260
  4      122  python-jobs-berlin-21-05-18          3.2 MB     1260
  5      119            software engineer          2.7 MB      774
  6      130                          aws          2.7 MB     1247
  7      127                    marketing          2.4 MB      956
  8      136                     database          2.3 MB      630
  9      124                      finance          2.3 MB      849
  10     114                      analyst          2.2 MB      873
  11     131                    developer          2.2 MB     1077
  12     112             business_analyst          1.7 MB      632
  13     113                       oracle        921.4 kB      417
  14     129                   internship        789.4 kB      356
  15      83                   e business        601.2 kB      160
  16      70                 intelligence        581.0 kB      148
  17      94                          sql        549.3 kB      158
  18      40        business intelligence        546.2 kB      138
  19     102                      startup        540.5 kB      143
  20      51                       system        536.3 kB      161
  21     178                       python        517.0 kB      146
  22      74                      finance        501.8 kB      156
  23      64                     engineer        492.5 kB      130
  24     110                     frontend        487.3 kB      128
  25      75                         html        459.5 kB      149
  26     103                         data        449.2 kB      125
  27     177            software engineer        442.6 kB      129
  28      89                          aws        439.1 kB      100
  29     141                            c        434.8 kB      148
  ..     ...                          ...             ...      ...
  117    120                junior_python         21.1 kB       98
  118    111                  live coding         19.6 kB        5
  119    168                       camera         18.9 kB        6
  120    140                        keras         17.5 kB        5
  121     22                       museum         17.2 kB        7
  122     57                 system admin         17.1 kB        5
  123     88                     beginner         17.0 kB        5
  124     23                advertisement         16.2 kB        5
  125      1                     anfänger         12.1 kB        4
  126    175                          dsp         11.3 kB        3
  127     62                growth hacker         11.1 kB        3
  128     32                       garden         10.9 kB        4
  129     27                        movie         10.6 kB        3
  130     60                      clojure         10.2 kB        3
  131     38             kunst und medien          9.6 kB        4
  132     65                     français          7.2 kB        2
  133     42               growth hacking          6.9 kB        2
  134     16                   buchhandel          6.3 kB        2
  135    158            assembly language          6.0 kB        1
  136     18               digital artist          5.7 kB        3
  137     50                   audio unit          4.7 kB        2
  138    157                          bsd          4.5 kB        1
  139     67                 lean analyst          4.4 kB        1
  140    165                         punk          3.4 kB        1
  141     72                        flask          3.1 kB        1
  142    142                         midi          3.0 kB        3
  143     84                       webapp          2.6 kB        1
  144     41                     fin tech          2.5 kB        1
  145    143                        d3.js          1.7 kB        1
  146     17                      flowers       900 Bytes        1
  
  [147 rows x 4 columns]
#+END_EXAMPLE
:END:

**** time evolution
**** return list for next scraper launch
***** remove null size results before (or not)
#+BEGIN_SRC ipython
queries_list = list(set(queries.name))
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[60]:
:END:
***** save in a file for editing
#+BEGIN_SRC ipython
with open("/queries/queries.txt", "w") as f:
    for query in queries_list:
        f.write(query + "\n")
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[67]:
:END:
*** launch scraper with the list
**** get list from file
#+BEGIN_SRC ipython
with open("queries/best.txt", "r") as f:
    queries_selected = f.read()
    queries_selected = queries_selected.splitlines()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[30]:
:END:

**** run shell script as subprocess
***** variables and imports
#+BEGIN_SRC ipython
import subprocess
from subprocess import Popen, PIPE
import shlex

cwd = '/home/teddd/data/projects/jobseeker/data/external/indeed/'
bash_script = [cwd + 'local_crawler_launch.sh']
arguments = queries_selected
command = bash_script + arguments
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[276]:
:END:

***** execution
****** stdout to buffer
#+BEGIN_SRC ipython
session = subprocess.Popen(command, stdout=PIPE, stderr=PIPE)
stdout, stderr = session.communicate()

if stderr:
    raise Exception("Error "+str(stderr))

stdout
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[255]:
: b'\n\n\nScraping query : flask \nrunning command : scrapy crawl job_offers_spider -a query=flask -o flask.csv\n\n\n\nScraping query : frontend \nrunning command : scrapy crawl job_offers_spider -a query=frontend -o frontend.csv\n\n\n\nScraping query : french \nrunning command : scrapy crawl job_offers_spider -a query=french -o french.csv\n\n\n\nScraping query : css \nrunning command : scrapy crawl job_offers_spider -a query=css -o css.csv\n\n\n\nScraping query : midi \nrunning command : scrapy crawl job_offers_spider -a query=midi -o midi.csv\n\n\n\nScraping query : startup \nrunning command : scrapy crawl job_offers_spider -a query=startup -o startup.csv\n\n\n\nScraping query : busine
ss intelligence \nrunning command : scrapy crawl job_offers_spider -a query=business intelligence -o business intelligence.csv\n\n\n\nScraping query : numpy \nrunning command : scrapy crawl job_offers_spider -a query=numpy -o numpy.csv\n\n\n\nScraping query : linux \nrunning command : scrapy crawl job_offers_spider -a query=linux -o linux.csv\n\n\n\nScraping query : e business \nrunning command : scrapy crawl job_offers_spider -a query=e business -o e business.csv\n\n\n\nScraping query : unix \n\nrunning command : scrapy crawl job_offers_spider -a query=unix -o unix.csv\n\n\nScraping query : pandas \nrunning command : scrapy crawl job_offers_spider -a query=pandas -o pandas.csv\n\n\n\nScraping query : github \nrunning command : scrapy crawl job_offers_spider -a query=github -o github.csv\n\n\n\nScraping query : data \n\n\n\nScraping query : online marketing \nrunning command : scrapy crawl job_offers_spider -a query=online marketing -o online marketing.csv\nrunning command : scrapy crawl job_offers_spider -a query=data -o data.csv\n\n\n\nScraping query : data scientist \nrunning command : scrapy crawl job_offers_spider -a query=data scientist -o data scientist.csv\n\n\n\nScraping query : intelligence \nrunning command : scrapy crawl job_offers_spider -a query=intelligence -o intelligence.csv\n\n\n\nScraping query : analyst \nrunning command : scrapy crawl job_offers_spider -a query=analyst -o analyst.csv\n\n\n\nScraping query : analyst \nrunning command : scrapy crawl job_offers_spider -a query=analyst -o analyst.csv\n\n\n\nScraping query : venture capital \nrunning command : scrapy crawl job_offers_spider -a query=venture capital -o venture capital.csv\n\n\n\nScraping query : html \n\n\n\nScraping query : backend \nrunning command : scrapy crawl job_offers_spider -a query=html -o html.csv\nrunning command : scrapy crawl job_offers_spider -a query=backend -o backend.csv\n\n\n\nScraping query : live coding \n\nrunning command : scrapy crawl job_offers_spider -a query=live coding -o live coding.csv\n\n\nScraping query : bsd \nrunning command : scrapy crawl job_offers_spider -a query=bsd -o bsd.csv\n\n\n\nScraping query : git \n\nrunning command : scrapy crawl job_offers_spider -a query=git -o git.csv\n\n\nScraping query : python \nrunning command : scrapy crawl job_offers_spider -a query=python -o python.csv\n\n\n\nScraping query : aws \nrunning command : scrapy crawl job_offers_spider -a query=aws -o aws.csv\n\n\n\nScraping query : sql \nrunning command : scrapy crawl job_offers_spider -a query=sql -o sql.csv\n\n\n\nScraping query : heroku \nrunning command : scrapy crawl job_offers_spider -a query=heroku -o heroku.csv\n\n\n\nScraping query : scraping \nrunning command : scrapy crawl job_offers_spider -a query=scraping -o scraping.csv\n\n\n\nScraping query : data visualization \n\n\n\nScraping query : marketing \nrunning command : scrapy crawl job_offers_spider -a query=data visualization -o data visualization.csv\nrunning command : scrapy crawl job_offers_spider -a query=marketing -o marketing.csv\n'
:END:

****** stdout to file
#+BEGIN_SRC ipython
from datetime import datetime
date = str(datetime.now())
with open("../data/external/crawl-log-" + date + ".txt",'w') as temp_file:
    crawl = subprocess.Popen(command, stdout=temp_file, cwd=cwd)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[278]:
:END:

** Filtering
*** Look for 1 keywords
**** keyword definiton
***** org variable
#+NAME: keyword
#+BEGIN_SRC python :nosession
"kunst und medien"
#+END_SRC

#+RESULTS: keyword
:RESULTS:
kunst und medien
:END:

**** look in title
***** boolean serie construction                                      :test:
#+BEGIN_SRC ipython :var k=keyword
df.title.str.contains(k, case=False)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[86]:
#+BEGIN_EXAMPLE
  3      False
  12     False
  14     False
  15     False
  19     False
  23     False
  27     False
  28     False
  35     False
  38     False
  45     False
  48     False
  55     False
  57     False
  59     False
  62     False
  63     False
  64     False
  65     False
  66     False
  75     False
  79     False
  82     False
  87     False
  91     False
  92     False
  93     False
  94     False
  96     False
  100    False
  ...
  44     False
  46     False
  49     False
  54     False
  55     False
  65     False
  68     False
  69     False
  70     False
  74     False
  77     False
  82     False
  84     False
  87     False
  89     False
  90     False
  93     False
  95     False
  96     False
  97     False
  102    False
  105    False
  109    False
  115    False
  116    False
  119    False
  121    False
  124    False
  126    False
  2      False
  Name: title, Length: 1623, dtype: bool
#+END_EXAMPLE
:END:

***** reduction of our dataset
#+BEGIN_SRC ipython :var k=keyword
    df = df[df.title.str.contains(k, case=False, na=False)]
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[92]:
:END:

**** look in description
#+BEGIN_SRC ipython :var k=keyword
    df = df[df.desc.str.contains(k, case=False, na=False)]
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[104]:
:END:

**** TODO test 
goto Johnny Kitchin
#+BEGIN_SRC ipython
k
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[94]:
: "# Out[91]:\n: 'database'"
:END:
*** Look for multiple keywords
**** tool: keywords list
use results from Queries
**** reduce dataframe
***** boolean serie
#+BEGIN_SRC ipython
df_bool = pd.DataFrame()
for query in queries_selected:
    df_bool[query] = df.desc.str.contains(query)
    #+END_SRC

    #+RESULTS:
    :RESULTS:
    # Out[75]:
    :END:

***** binary serie
#+BEGIN_SRC ipython
def bool_to_bin(x):
    if x is True:
        return 1
    else:
        return 0

df_bin = pd.DataFrame()

for query in queries_selected:
    df_bin[query] = df_bool[query].apply(bool_to_bin)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[76]:
:END:

***** score attribution

****** overview
#+BEGIN_SRC ipython
pd.concat({"title":df.title, "score":df_bin.sum(axis=1)}, axis=1).sort_values("score", ascending=False)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[121]:
#+BEGIN_EXAMPLE
  score                                              title
  1       10           Head of Software Development // mediaire
  120      7         Senior Business Intelligence Analyst (m/f)
  110      6               Backend Developer (m/f) // Piloteers
  12       5                             Communications Manager
  51       5                  Frontend Engineer (m/f) // deevio
  22       5                           Lead Engineer Full Stack
  25       5                                Lead Data Scientist
  62       5              Senior Business Intelligence Engineer
  42       5   Conversion Rate Optimization (CRO) Manager (m/f)
  55       5                         IT Technical Support (m/w)
  113      5                    Backend Software Engineer (m/f)
  5        5                 Growth Hacker/Performance Marketer
  79       5                 Junior B2B Marketing Manager (f/m)
  68       5                           Software Developer (f/m)
  99       5  Analytics - Internship (m/f) for at least 6 mo...
  53       5  Senior Financial Planning & Analysis Controlle...
  124      5  Business Intelligence Analyst (f/m), Global BI...
  61       5                        Marketing Manager B2B (m/f)
  8        5                    Business Development internship
  19       5                Senior Ruby Engineer // Priori Data
  10       5                       Senior UX Researcher (f/m/d)
  105      5            (Senior) Data Analyst - Marketing (m/f)
  104      4                            Marketing Analyst (m/f)
  130      4         Big Data Engineer (m/f) (Part-or Fulltime)
  147      4    Intern (m/f) Sales Analyst – Europe // Applause
  22       4                         Frontend Developer / React
  103      4                             Intern/Working Student
  2        4  Front-End Developer / Financial Reporting & Da...
  59       4                         Back-End Software Engineer
  40       4           Senior Data Analyst (m/w) - HelloFreshGO
  ..     ...                                                ...
  60       0                                             Intern
  51       0     Praktikum Marketing Bio-Lebensmittel in Berlin
  112      0                             Network Engineer (m/w)
  49       0     Grafiker ab sofort (m/w) möglichst in Teilzeit
  129      0         Marketing- und Kommunikationsmanager (w/m)
  133      0                     Online Marketing Manager (m/w)
  134      0                  Ausbildung Marketingkommunikation
  8        0                             Fachinformatiker (m/w)
  110      0                  Business Analyst Commercial (m/w)
  108      0       Datenmanager (m/w) für Portfoliomodellierung
  22       0                 Data Scientist (m/w) Industrie 4.0
  96       0   Mitarbeiter IT-Support und -Administration (m/w)
  85       0              Senior Inhouse Consultant (m/w) (SRM)
  82       0  IT-Betreuer (w/m) für das Evangelische Waldkra...
  19       0                           Android Entwickler (m/w)
  79       0             Werkstudent Operations Analytics (w/m)
  71       0  Head of Supply Analysis and Logistics (m/w) be...
  20       0                  (Senior) Consultant: DevOps (m/w)
  21       0                           Webentwickler (m/w) Java
  22       0  Softwareentwickler Java (w/m) für Indexplattfo...
  50       0                      Pricing & Yield Manager (m/w)
  46       0                       Werkstudent Informatik (m/w)
  36       0  Web Developer (m/w) mit Drupal Kenntnissen ges...
  42       0                   Senior PHP-Entwickler m:w Berlin
  39       0    Product Data Manager (m/w) E-Commerce in Berlin
  44       0         Java Developer / Software Entwickler (m/w)
  34       0                                QA Engineer (m/f/x)
  27       0        Projektleiter (m/w) Lagerverwaltungssysteme
  18       0  Praktikant / Werkstudent (m/w) in der Full-Sta...
  0        0  Praktikant Einführung Business Intelligence-An...
  
  [934 rows x 2 columns]
#+END_EXAMPLE
:END:

****** reduce dataframe for visual exploration
#+BEGIN_SRC ipython
df_print = df
df_print["score"] = df_bin.sum(axis=1)
df_print = df_print.sort_values("score", ascending=False)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[122]:
:END:

**** guide: used words
***** amongst keywords
#+BEGIN_SRC ipython
df_bin.sum().sort_values(ascending=False)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[101]:
#+BEGIN_EXAMPLE
  git                      301
  data                     243
  marketing                105
  startup                  105
  backend                   72
  e business                64
  frontend                  50
  intelligence              44
  analyst                   22
  data scientist            16
  venture capital           12
  data visualization         9
  business intelligence      8
  online marketing           8
  python                     6
  html                       5
  sql                        5
  aws                        4
  french                     4
  github                     4
  numpy                      3
  pandas                     3
  linux                      2
  flask                      2
  css                        2
  live coding                2
  bsd                        1
  midi                       1
  unix                       0
  scraping                   0
  heroku                     0
  dtype: int64
#+END_EXAMPLE
:END:

**** which contains most of the querie keywords ?  
**** add weight to keywords ?
**** keywords distance map
with all keywords, you are at the center
*** companies
#+BEGIN_SRC ipython
df = df[df.company.str.contains("berlin", case=False, na=False)]
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[86]:
:END:
** Stats
*** overview
**** head
#+BEGIN_SRC ipython
df.head()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[21]:
#+BEGIN_EXAMPLE
  Empty DataFrame
  Columns: [location, related, title, url, company, days_ago, contract, desc]
  Index: []
#+END_EXAMPLE
:END:

**** count
#+BEGIN_SRC ipython
len(df)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[106]:
: 0
:END:

*** days ago
**** histogram
***** pd plot
#+BEGIN_SRC ipython
    df.days_ago.plot.hist()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[82]:
: <matplotlib.axes._subplots.AxesSubplot at 0x7f1136869c18>
[[file:./obipy-resources/TtBMu6.png]]
:END:
**** value count
#+BEGIN_SRC ipython
    df.days_ago.value_counts()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[41]:
#+BEGIN_EXAMPLE
  3     136
  1     125
  9     115
  2     109
  8      80
  7      74
  4      71
  10     70
  23     68
  11     68
  14     68
  24     57
  17     56
  16     55
  21     55
  18     52
  15     48
  22     47
  25     46
  28     40
  6      39
  29     38
  13     35
  12     28
  5      23
  27     20
  20     19
  19     16
  26     13
  46      1
  56      1
  Name: days_ago, dtype: int64
#+END_EXAMPLE
:END:
**** groupby
***** basic output
#+BEGIN_SRC ipython
    df.groupby(["days_ago"]).groups
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[54]:
#+BEGIN_EXAMPLE
  {1: Int64Index([20, 25, 49, 136], dtype='int64'),
  2: Int64Index([2, 4, 10, 30, 71, 77, 116, 125, 139], dtype='int64'),
  3: Int64Index([27, 54, 73, 98, 106, 128], dtype='int64'),
  4: Int64Index([29, 32, 60, 97, 114, 119, 143], dtype='int64'),
  5: Int64Index([50, 135], dtype='int64'),
  6: Int64Index([129], dtype='int64'),
  7: Int64Index([127], dtype='int64'),
  8: Int64Index([104, 112, 113, 121, 138], dtype='int64'),
  9: Int64Index([142], dtype='int64'),
  10: Int64Index([3, 96], dtype='int64'),
  11: Int64Index([86, 132], dtype='int64'),
  12: Int64Index([109], dtype='int64'),
  13: Int64Index([31], dtype='int64'),
  14: Int64Index([22, 24, 95], dtype='int64'),
  16: Int64Index([47], dtype='int64'),
  17: Int64Index([6, 37, 41], dtype='int64'),
  18: Int64Index([80], dtype='int64'),
  20: Int64Index([79], dtype='int64'),
  21: Int64Index([55], dtype='int64'),
  22: Int64Index([1, 144], dtype='int64'),
  23: Int64Index([21, 52, 75, 110], dtype='int64'),
  24: Int64Index([66, 67], dtype='int64'),
  25: Int64Index([14], dtype='int64'),
  26: Int64Index([91], dtype='int64'),
  27: Int64Index([48], dtype='int64'),
  29: Int64Index([145], dtype='int64')}
#+END_EXAMPLE
:END:
***** loop print
#+BEGIN_SRC ipython
grouped = df.groupby("days_ago")

for name,group in grouped:
    print(name)
    print(group)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[52]:
:END:
** Printing
*** quick overview
**** head
#+BEGIN_SRC ipython
df.head()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[5]:
#+BEGIN_EXAMPLE
  location                                            related  \
  0   Berlin  https://de.indeed.com/Python-Developer-Jobs-in...
  1   Berlin                                                NaN
  2   Berlin  https://de.indeed.com/Senior-Software-Tester-J...
  3   Berlin  https://de.indeed.com/Lead-Product-Analyst-Job...
  4   Berlin  https://de.indeed.com/Softwareentwickler-Entwi...
  
  title  \
  0                             Python Developer (m/w)
  1                            Software-Entwickler w/m
  2                       Senior Software-Tester (w/m)
  3                               Lead Product Analyst
  4  Softwareentwickler (m/w) für Entwicklungsumgeb...
  
  url        company days_ago  \
  0  https://de.indeed.com/viewjob?jk=05f2b8ca5157f...  Bidmanagement      30+
  1  https://de.indeed.com/cmp/Qtixx-GmbH/jobs/Soft...     Qtixx GmbH       22
  2  https://de.indeed.com/viewjob?jk=d9b44d35ab5be...    Carmeq GmbH        2
  3  https://de.indeed.com/viewjob?jk=9b572e61f1945...      eBay Inc.       10
  4  https://de.indeed.com/viewjob?jk=c181a1609f4bb...    Carmeq GmbH        2
  
  contract                                               desc
  0       NaN  <span id="job_summary" class="summary"><div><d...
  1       NaN  <span id="job_summary" class="summary"><p>Die ...
  2       NaN  <span id="job_summary" class="summary"><div><d...
  3       NaN  <span id="job_summary" class="summary"><div><p...
  4       NaN  <span id="job_summary" class="summary"><div><d...
#+END_EXAMPLE
:END:

**** count
#+BEGIN_SRC ipython
df.title.count()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[6]:
: 146
:END:
**** titles
#+BEGIN_SRC ipython
df.title
#+END_SRC

*** html pages
**** hacked around solution                                            :test:
***** function to save results to html
#+NAME: html-save
#+BEGIN_SRC ipython
    from datetime import datetime
    from os import mkdir

    def htmlexport(df, begin, end):
                date = str(datetime.now())
                path = "../reports/html/" + date + "/"
                mkdir(path)
                for i in range(begin, end):
                            html = ""
                            html = html + "\n"
                            html = html + "Job number " + str(i)
                            html = html + "\n"
                            html = html + "-"*100
                            html = html + "\n" + df.title.iloc[i]
                            html = html + "\n"
                            html = html + df.company.iloc[i]
                            html = html + "\n"
                            html = html + "-"*100
                            html = html + "\n"
                            html = html + df.desc.iloc[i]
                            html = html + "\n"*3
                            html = html + "-"*100
                            html = html + "\n"*3
                            filename = path + "job-" + str(i) + ".html"
                            with open(filename, "a") as file:
                                        file.write(html)
#+END_SRC

***** call function
#+BEGIN_SRC ipython
    htmlexport(dfk, 0, dfk.title.count())
#+END_SRC
***** PB : imossible to add links because of some encoding pb
**** use xml.dom                                                       :test:
***** use
#+BEGIN_SRC ipython 
    from xml.dom import minidom
    minidom.parseString(dfk.desc.iloc[10])
#+END_SRC

***** PB : some descs are separated by comas
****** change spider
****** use regexp to parse again
****** test with proper html files : maybe it is just not working with html ?
#+BEGIN_SRC ipython 
    from xml.dom import minidom
    minidom.parseString("~/code/web/plasma-city/application/static/front.html")
#+END_SRC

**** use yattag
***** imports
#+BEGIN_SRC ipython
    from datetime import datetime
    from os import mkdir
    from yattag import Doc
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[142]:
:END:

***** html page generation
****** functions definition
#+BEGIN_SRC ipython
def linksgen(filename_base, pagenum, url):
    doc, tag, text = Doc().tagtext()

    with tag("div"):
        with tag('a', href = "."):
            text('Home page')
        with tag("div"):
            with tag("a", href = filename_base + str(pagenum - 1) + ".html"):
                text("Previous page")
            text(" ")
            with tag("a", href = filename_base + str(pagenum + 1) + ".html"):
                text("Next page")
        with tag("a", href = url, target="_blank"):
            text("Original page")
            
    return doc.getvalue()


def pagegen(filename_base, pagenum, title, desc, company, days, url):
    doc, tag, text = Doc().tagtext()
    
    doc.asis('<meta charset="UTF-8">')
    with tag("title"):
        text(title)
    with tag("body"):
        doc.asis(linksgen(filename_base, pagenum, url))
        with tag("h1"):
            text(title)
        with tag("h2"):
            text(company)
        with tag("p"):
            text(str(days) + " days ago")
        with tag("div"):
            doc.asis(desc)
        doc.asis(linksgen(filename_base, pagenum, url))

    return doc.getvalue()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[143]:
:END:

****** test pagegen                                                  :test:
#+BEGIN_SRC ipython
pagegen("nom", 0, "titre", "desc", "firm", "days", "www")
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[144]:
: '<meta charset="UTF-8"><title>titre</title><body><div><a href=".">Home page</a> <a href="nom-1.html">Previous page</a> <a href="nom1.html">Next page</a> <a href="www" target="_blank">Original page</a></div><h1>titre</h1><h2>firm</h2><p>days days ago</p><div>desc</div><div><a href=".">Home page</a> <a href="nom-1.html">Previous page</a> <a href="nom1.html">Next page</a> <a href="www" target="_blank">Original page</a></div></body>'
:END:

****** test linksgen                                                 :test:
#+BEGIN_SRC ipython
linksgen("file", 10, "wwwww")
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[145]:
: '<div><a href=".">Home page</a> <a href="file9.html">Previous page</a> <a href="file11.html">Next page</a> <a href="wwwww" target="_blank">Original page</a></div>'
:END:
***** htmlexport function
****** definition
#+BEGIN_SRC ipython
def htmlexport(df, begin, end):
    date = str(datetime.now())
    path = "../reports/html/" + date + "/"
    mkdir(path)
    for i in range(begin, end):
        filename_base = "job-"
        html = pagegen(filename_base,
                       i,
                       df.title.iloc[i],
                       df.desc.iloc[i],
                       df.company.iloc[i],
                       df.days_ago.iloc[i],
                       df.url.iloc[i]
        )
        filename = path + filename_base +  str(i) + ".html"
        with open(filename, "a") as file:
            file.write(html)

#+END_SRC

#+RESULTS:
:RESULTS:
# Out[146]:
:END:

****** call
#+BEGIN_SRC ipython
htmlexport(df_print, 0, 40)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[147]:
:END:

****** link
file:home/teddd/data/projects/jobseeker/reports/html/
*** server
**** flask ? :D !!!
*** org  table (python)                                               :python:
**** john kitchin example                                        :test:
#+BEGIN_SRC python
    import pandas as pd
    test = pd.DataFrame({'A': [1000, 1000], 'B' : [60, 100]})
    test2 = [list(test)] + [None] + test.values.tolist()
    test3 = test.values.tolist()
    return (test, test2, test3)
#+END_SRC

**** my program                                                  :slow:
#+NAME: data-set
#+BEGIN_SRC python :var data=data-path
    import pandas as pd
    df = pd.read_csv(data)
    return  [list(df)] + [None] + df.values.tolist()
#+END_SRC

**** COMMENT in an org table                                           :slow:
#+BEGIN_SRC ipython :eval no
    head = df.head()
    [list(head)] + [None] + head.values.tolist()
#+END_SRC

                                                               :test:
*** org results: html                                                   :test:
#+BEGIN_SRC python :results html
    dfk.desc.iloc[0]
#+END_SRC

*** soupprint
**** session functions
***** souper (using get text)
#+BEGIN_SRC ipython
from bs4 import BeautifulSoup

def souper(html):
    "returns only the text from a html string"
    soup = BeautifulSoup(html, 'html.parser')
    return soup.get_text()
    #+END_SRC

    #+RESULTS:
    :RESULTS:
    # Out[13]:
    :END:
***** soupprint
****** definition
#+BEGIN_SRC ipython
    from bs4 import BeautifulSoup

    def souper(html):
        soup = BeautifulSoup(html, 'html.parser')
        print(soup.get_text())

    def soupprint(df, begin, end):
        for i in range(begin, end):
            print(i, df.title.iloc[i])
            print("\n")
            print(df.company.iloc[i])
            print("\n")
            souper(df.desc.iloc[i])
            print("\n"*3)
            print("-"*100)
            print("\n"*3)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[14]:
:END:
****** call
#+BEGIN_SRC ipython
soupprint(df, 0, 10)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[15]:
:END:

**** soupprint as org function
***** definition
#+NAME: soupprint
#+BEGIN_SRC python
from bs4 import BeautifulSoup

def souper(html):
    soup = BeautifulSoup(html, 'html.parser')
    print(soup.get_text())

def soupprint(df, begin, end):
    for i in range(begin, end):
        print(i, df.title.iloc[i])
        print("\n")
        print(df.company.iloc[i])
        print("\n")
        souper(df.desc.iloc[i])
        print("\n"*3)
        print("-"*100)
        print("\n"*3)
#+END_SRC

#+RESULTS: soupprint
:RESULTS:
:END:

***** call
#+CALL: soupprint()

#+RESULTS[035511d92ded44ec24cc84fea0b5511c5863b3b6]:

#+BEGIN_SRC python 
    soupprint(dfk, 0, dfk.title.count())
#+END_SRC
* External Documentation
** doc : look for matching patern                                         :doc:
#+BEGIN_SRC ipython :eval no
help(df.title.str.contains)
#+END_SRC

#+RESULTS:
#+begin_example
Help on method contains in module pandas.core.strings:

contains(pat, case=True, flags=0, na=nan, regex=True) method of pandas.core.strings.StringMethods instance
    Return boolean Series/``array`` whether given pattern/regex is
    contained in each string in the Series/Index.
    
    Parameters
    ----------
    pat : string
        Character sequence or regular expression
    case : boolean, default True
        If True, case sensitive
    flags : int, default 0 (no flags)
        re module flags, e.g. re.IGNORECASE
    na : default NaN, fill value for missing values.
    regex : bool, default True
        If True use re.search, otherwise use Python in operator
    
    Returns
    -------
    contained : Series/array of boolean values
    
    See Also
    --------
    match : analogous, but stricter, relying on re.match instead of re.search
#+end_example

** pandas
[[~/Cours/Data/cheat-sheets/Pandas_Cheat_Sheet.pdf][Pandas cheat sheet]]
* Tests
** ob-ipython
*** hands-on tryout
:PROPERTIES:
:header-args: :session test
:END:
**** hello world
#+BEGIN_SRC ipython
print 'hello world'
#+END_SRC
**** function definition
#+BEGIN_SRC ipython
    def fn():
        print "I am in the session !"
#+END_SRC

**** function call
#+BEGIN_SRC ipython
fn()
#+END_SRC

*** doc tutorial
:PROPERTIES:
:header-args: :session other :results raw drawer
:END:
**** imports
#+BEGIN_SRC ipython
  %matplotlib inline
  import matplotlib.pyplot as plt
  import numpy as np
#+END_SRC

**** ex2
#+BEGIN_SRC ipython
  def foo(x):
      return x + 9

  [foo(x) + 7 for x in range(7)]
#+END_SRC

**** images
***** ex1
#+BEGIN_SRC ipython :exports both
  plt.hist(np.random.randn(20000), bins=200)
#+END_SRC

***** ex2
#+BEGIN_SRC ipython :ipyfile /tmp/image.png :exports both :results raw drawer
  plt.hist(np.random.randn(20000), bins=200)
#+END_SRC

***** config
#+BEGIN_SRC ipython
%config InlineBackend.figure_format = 'svg'
#+END_SRC

**** other kernel
#+BEGIN_SRC ipython :session clojure :kernel clojure
  (+ 1 2)
#+END_SRC

**** async
#+BEGIN_SRC ipython :ipyfile /tmp/image.png :exports both :async t :results raw drawer
  import time
  time.sleep(3)
  plt.hist(np.random.randn(20000), bins=200)
#+END_SRC

*** other tryouts
**** functions
:PROPERTIES:
:header-args: :session neuf :results raw drawer
:END:
***** definition                                                  :noexport:
#+BEGIN_SRC ipython
    def lol():
        /print "This is the fun !"
#+END_SRC

***** call
#+BEGIN_SRC ipython 
lol()
#+END_SRC

**** formater
:PROPERTIES:
:header-args: :session formater  :results raw drawer
:END:
***** init
#+BEGIN_SRC ipython
import IPython
from tabulate import tabulate

class OrgFormatter(IPython.core.formatters.BaseFormatter):
    def __call__(self, obj):
        try:
            return tabulate(obj, headers='keys',
                            tablefmt='orgtbl', showindex='always')
        except:
            return None

ip = get_ipython()
ip.display_formatter.formatters['text/org'] = OrgFormatter()
#+END_SRC
***** arrays

**** kernel tests
***** session header arg after run console
#+BEGIN_SRC ipython :session xxx
print("hello")
#+END_SRC
***** kernel headerarg
#+BEGIN_SRC ipython :session :kernel python3
print("hello")
#+END_SRC
** nltk
:PROPERTIES:
:header-args: :session explorer :results raw drawer
:END:
*** text selection
**** sample text base
#+BEGIN_SRC ipython
from nltk.book import *
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[8]:
:END:
**** access text as string
***** imports
#+BEGIN_SRC ipython
import nltk, re, pprint
from nltk import word_tokenize
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[9]:
:END:
***** with one description
****** definition
#+BEGIN_SRC ipython
string = df.iloc[0].desc
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[77]:
:END:

****** formating
******* html 
#+BEGIN_SRC ipython
string = souper(string)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[78]:
:END:

******* case
#+BEGIN_SRC ipython
string = string.lower()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[79]:
:END:

******* punctiations

******** definition
#+BEGIN_SRC ipython
def multi_replace(string, *args, replace=" "):
    for target in args:
        string = string.replace(target, replace)
    return string

trash_car = (",", "\'", "\"", "&", "#", "{", "}",
             "(", ")", "[", "]", "_", "\\", "~", "-",
             ",", ";", ":", ".", "?", "!", "+", "|",
             "@", "/", "–", "*", "“", "„", "%", " ",
             "€")

#+END_SRC

#+RESULTS:
:RESULTS:
# Out[80]:
:END:

******** call
#+BEGIN_SRC ipython
string = multi_replace(string, *trash_car)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[81]:
:END:


***** to ntlk text object
****** tokenizing
#+BEGIN_SRC ipython
tokens = word_tokenize(string)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[83]:
:END:

****** use as nltk text
#+BEGIN_SRC ipython
text = nltk.Text(tokens)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[84]:
:END:
*** search
**** concordance
#+BEGIN_SRC ipython
text.concordance("data")
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[98]:
:END:
**** similar word
#+BEGIN_SRC ipython
text.similar("analyst")
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[100]:
:END:
**** dispersion
#+BEGIN_SRC ipython
text.dispersion_plot(["up", "with", "in", "the", "for", "team"])
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[101]:
[[file:./obipy-resources/l01HMt.png]]
:END:
*** generation                                                          :test:
#+BEGIN_SRC ipython
text.generate(["The", "job", "is", "for", "data", "team"])
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[110]:
:END:

*** normalizing
**** steaming
**** lemmatization
*** vocabulary
**** sorted set
#+BEGIN_SRC ipython
sorted(set(text))
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[111]:
#+BEGIN_EXAMPLE
  ['17',
  '2008',
  '23',
  '3',
  '40',
  '5',
  '<',
  'a',
  'able',
  'about',
  'academic',
  'across',
  'active',
  'adapt',
  'additional',
  'advertising',
  'all',
  'also',
  'an',
  'analysis',
  'analysts',
  'analytical',
  'analytics',
  'analyzing',
  'and',
  'anja',
  'are',
  'area',
  'art',
  'as',
  'aspects',
  'assistance',
  'at',
  'atmosphere',
  'attitude',
  'available',
  'backgrounds',
  'basis',
  'behavior',
  'beverages',
  'bieten',
  'brands',
  'bringing',
  'building',
  'business',
  'but',
  'celebrate',
  'centrally',
  'challenges',
  'change',
  'changing',
  'choose',
  'closely',
  'coaching',
  'com',
  'come',
  'commerce',
  'committed',
  'communicating',
  'comparable',
  'competitive',
  'computer',
  'conflicts',
  'connecting',
  'context',
  'contribute',
  'crm',
  'customer',
  'customers',
  'daily',
  'data',
  'databases',
  'de',
  'decided',
  'decisions',
  'deep',
  'department',
  'develop',
  'developing',
  'development',
  'different',
  'digital',
  'direct',
  'discount',
  'discounts',
  'diverse',
  'diversity',
  'drive',
  'e',
  'easily',
  'economics',
  'effectively',
  'ein',
  'einkaufserlebnis',
  'employee',
  'employment',
  'empowerment',
  'enable',
  'entire',
  'environment',
  'equipment',
  'equivalent',
  'europas',
  'europe',
  'experience',
  'expertise',
  'experts',
  'external',
  'fashion',
  'fast',
  'feedback',
  'field',
  'flexible',
  'focus',
  'for',
  'foundation',
  'free',
  'from',
  'fruits',
  'full',
  'führende',
  'g',
  'getting',
  'great',
  'groups',
  'have',
  'head',
  'health',
  'help',
  'holidays',
  'https',
  'ideally',
  'impact',
  'in',
  'independently',
  'information',
  'innovations',
  'insights',
  'inspiring',
  'intelligence',
  'interests',
  'internal',
  'international',
  'internationals',
  'into',
  'is',
  'ist',
  'it',
  'its',
  'junior',
  'keep',
  'knowledge',
  'kunden',
  'languages',
  'lay',
  'lead',
  'leading',
  'located',
  'logistics',
  'long',
  'look',
  'looking',
  'lounge',
  'mail',
  'managing',
  'many',
  'markets',
  'mathematics',
  'means',
  'members',
  'mentoring',
  'merit',
  'methods',
  'million',
  'models',
  'more',
  'most',
  'municipality',
  'name',
  'need',
  'needed',
  'new',
  'not',
  'of',
  'off',
  'offering',
  'offerings',
  'offices',
  'on',
  'online',
  'only',
  'opensource',
  'opportunities',
  'or',
  'other',
  'our',
  'out',
  'paced',
  'partners',
  'perks',
  'personal',
  'perspectives',
  'platform',
  'plattform',
  'positive',
  'potential',
  'priorities',
  'proactive',
  'public',
  'python',
  'qualifications',
  'questions',
  'quick',
  'r',
  'radar',
  're',
  'recruiter',
  'related',
  'relevant',
  'relocation',
  'reports',
  'represent',
  'research',
  'responsibility',
  'run',
  's',
  'salary',
  'science',
  'segment',
  'seit',
  'senior',
  'services',
  'sets',
  'share',
  'shop',
  'shopping',
  'showcases',
  'similar',
  'size',
  'skill',
  'skills',
  'solutions',
  'solve',
  'spierling',
  'sports',
  'sql',
  'stakeholders',
  'state',
  'statistical',
  'statistics',
  'strategy',
  'structures',
  'subject',
  'tailored',
  'team',
  'teams',
  'tech',
  'technical',
  'term',
  'than',
  'that',
  'the',
  'their',
  'them',
  'things',
  'thinking',
  'through',
  'time',
  'times',
  'to',
  'toe',
  'together',
  'too',
  'top',
  'transforming',
  'transport',
  'trust',
  'umfassendes',
  'understanding',
  'unseren',
  'up',
  'use',
  'user',
  'using',
  'valuable',
  'variety',
  'volunteering',
  'warehouse',
  'ways',
  'we',
  'well',
  'what',
  'where',
  'which',
  'will',
  'wir',
  'with',
  'work',
  'working',
  'workplace',
  'x',
  'years',
  'you',
  'your',
  'zalando']
#+END_EXAMPLE
:END:
**** lexical richness
***** tryout
#+BEGIN_SRC ipython
len(text) / len(set(text))
#+END_SRC

***** function
#+BEGIN_SRC ipython
def lexical_diversity(text):
    return len(text) / len(set(text))
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[113]:
: 1.6950819672131148
:END:
**** specific word
***** tryout
#+BEGIN_SRC ipython
100 * text.count('for') / len(text)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[117]:
: 1.5473887814313345
:END:
***** functyion
#+BEGIN_SRC ipython
def word_percentage(word):
    return 100 * text.count(word) / len(text)
#+END_SRC

*** TODO Build a corpus !

**** sklearn
#+BEGIN_SRC ipython
docs = df['desc']

tfs = tfidf.fit_transform(docs)
#+END_SRC
